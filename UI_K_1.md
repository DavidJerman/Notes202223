# Umetna inteligenca

## Pred snovjo

Sekcije s pojmi imajo na voljo check-marke. Le-te lahko oznaÄiÅ¡ tako, da v oglate oklepaje namesto presledka vstaviÅ¡
`x`. Na primer: `[x]` oznaÄi tole. Tako si lahko daÅ¡ zaznamke, kaj si Å¾e prebral.

## Snov

### Uvod

#### Kaj je umetna inteligenca?

Strojno oprema z inteligentnim vedenjem. Sistemu, ki pa odraÅ¾a inteligenco pa reÄemo agent.
Ko neki AI zna reÅ¡iti problem reÄemo uÄinek AI - problem je reÅ¡ljiv z algoritmom, torej ni veÄ AI.

Pojmi:

- [ ] Ozka (Å¡ibka) vs sploÅ¡na (moÄna) AI
- [ ] ToÄka singularnosti, superinteligenca
- [ ] Turingov test

### ReÅ¡evanje problemov z iskanjem

ReÅ¡evanje problema je v bistvu iskanje poti od zaÄetnega stanja do cilja, Äe ustvarimo prostor stanj.
Tako lahko za reÅ¡evanje takih problemov uporabimo iskalne algoritme: DFS, BFS, A*...
Hevristika nam poda neko znanje, oceno o okolju v katerem smo, tako da lahko
bolj uÄinkovito preiskujemo prostor stanj.

Prostor stanj <N, A, S, G>:

- N - mnoÅ¾ica vseh stanj
- A - mnoÅ¾ica vseh akcij
- S - zaÄetno stanje
- G - ciljno stanje

Opis stanja je lahko popolni ali delni. ÄŒe je delen samo pomeni, da nimamo vseh
informacij o trenutnem stanju.

Cilj iskanja je lahko **najti pot do cilja** ali **najti ciljno stanje**.
Prvo je npr. iskanje poti do doma, drugo pa reÅ¡itev sudokuja.

Prostor stanj lahko vizualiziramo z grafom - stanja so vozliÅ¡Äa, povezave pa akcije.
Imamo cikliÄen, acikliÄen graf in drevo. Globina d je oddaljenost od zaÄetnega vozliÅ¡Å¡Äa
do konÄnega vozliÅ¡Äa. Vejitveni faktor b je Å¡tevilo povezav, ki vodijo iz enega
vozliÅ¡Äa. Ponavadi vzamemo povpreÄje.

#### Iskalno drevo

NaÄini iskanja:

- Iskanje z veriÅ¾enjem naprej,
- Iskanje z veriÅ¾enjem nazaj,
- Iskanje z veriÅ¾enjem naprej in nazaj (dvosmerno).

Potek iskanja lahko ponazorimo z iskalnim drevesom. Razvoj vozliÅ¡Äa pomeni, da tvorimo
vse naslednike.

<img src="https://davidblog.si/wp-content/uploads/2023/04/Screenshot-2023-04-01-144356.png" width="700" alt="Drevo stanj">

#### Implementacija iskanja

Uporabimo seznama open in closed, ter iskanje naprej.
V open imamo obiskana vozliÅ¡Äa. V closed pa razvita vozliÅ¡Äa.

Poznamo dve vrsti strategij:

- [ ] Informirana - uporabljamo hevristiko
- [ ] Neinformirana - ne uporabljamo hevristike

Algoritem je poln, Äe reÅ¡itev vedno najde in optimalen, Äe najde najboljÅ¡o reÅ¡itev.

#### Neinformirane strategije

- [ ] BFS - iskanje v Å¡irino, _open_ je FIFO vrsta, optimalen, poln, O(b^d+1)
- [ ] BFS z uniformnih stroÅ¡kom - _open_ urejen po stroÅ¡ku poti, poln, optimalen, Äe povezave vse +
- [ ] DFS - iskanje v globino, _open_ je LIFO vrsta, ni optimalen, je poln, O(b^m), m je max. razdalja med dvema
  vozliÅ¡Äema v grafu prostora stanj
- [ ] DFS z omejitvijo globine - se omejimo do globine l, ni nujno poln, Äe ne gremo dovolj globoko, O(b^l)
- [ ] DFS z omejitvijo globine (postopno poveÄevanje globine) - Å¡e najbolj primerno kot neinformirana strategija
- [ ] Dvosmerno iskanje - iskanje naprej in nazaj, O(b^(d/2)) - stik preverjamo s presekom open

#### Informirane strategije

Uporaba hevristike - hevristiÄno iskanje! ReÅ¡evanje optimizacijskih problemov.

- [ ] Best-first search - v vsakem koraku izberemo vozliÅ¡Äe z najboljÅ¡o oceno f(n) v seznamu _open_ - seznam urejen po
  f(n)

Osnovna komponenta: hevristiÄna ocena h(n) - ocena stroÅ¡ka do cilja. Npr. zraÄna razdalja
do cilja.

#### Algoritem A

Za izbiro naslednje poteze uporabimo oceno f(n) = g(n) + h(n), kjer je g(n) pot do trenutnega vozliÅ¡Äa,
h(n) pa ocena stroÅ¡ka do cilja. Ni ravno idealen.

#### Algoritem A*

Algoritem A* je izboljÅ¡ava algoritma A. Predvsem gre za to, da uporabimo boljÅ¡o hevristiko.
Ta mora biti:

- sprejemljiva (moramo podceniti stroÅ¡ek) - h(n) <= h*(n), kjer je h*(n) optimalna hevristika
- za vsako vozliÅ¡Äe open in closed beleÅ¾imo najkrajÅ¡o pot. ÄŒe je nova pot krajÅ¡a, potem
  vozliÅ¡Äe dodamo v open - odstranimo starega oz. posodobimo njegovo oceno. ÄŒe je vozliÅ¡Äe
  v closed, ga odstranimo in ga dodamo v open ter popravimo Å¡e vsem naslednikom ALI (!!!!!!)
- hevristiÄna ocena je konsistentna oz. monotona: h(n) <= c(n, n') + h(n), torej
  velja trikotniÅ¡ka neenakost

Ta algoritem je poln in optimalen.

IzboljÅ¡ave: postopno poglabljanje, rekurzivno iskanje z izbiro najboljÅ¡ega, pomnilniÅ¡ko omejen A*, ...

[A* vs DFS vs BFS](https://github.com/DavidJerman/AStar)

#### Hevristika

Primeri hevristik:

- [ ] ZraÄna razdalja
- [ ] Å tevilo kock na nepravem/pravem mestu

Poglej dodatno: pojmi na strani 71/106.

#### Lokalni iskalni algoritem

Ideja je, da se premikamo iz stanja v stanje, brez da bi poznali okolico. Preko tega spoznamo okolico.
Skratka izvajamo lokalne spremembe in iÅ¡Äemo globalni maksimum (Äeprav ponavadi najdemo nek
lokalni maksimum).

Primerno, kadar iÅ¡Äemo stanja z maksimalno ugodnostjo (fitness). To ugodnost dobimo
s pomoÄjo hevristiÄne ocenitvene funkcije.

<img src="https://www.tutorialandexample.com/wp-content/uploads/2019/07/Working-of-a-Local-search-algorithm.png" width="700" alt="Lokalni iskalni algoritem">

Primer je lahko sudoku, kjer je premik zamenjava dveh nefiksnih polj, cilj pa je najti
konÄno reÅ¡itev - optimalna reÅ¡itev ima hevristiÄno oceno 0.

##### Hill climbing

ZaÄnemo iz nakljuÄnega stanja, izberemo potezo, ki prinaÅ¡a najveÄje izboljÅ¡anje trenutnega stanja.
ZakljuÄimo, ko najdemo optimalno reÅ¡itev (globalni maksimum oz. optimum).
Problem platoja (ravnine) omejimo tako, da omejimo Å¡tevilo premikov na ravnini.

IzboljÅ¡ave: stohastiÄno vzpenjanje glede na hevristiko, s prvim izborom, z nakljuÄno
ponovitvijo - nekajkrat zaÄnemo znova iz novega nakljuÄnega stanja, ...

##### Simulated annealing

Kombiniramo vzpenjanje na hrib z nakljuÄnim iskanjem - polnost iskanja.
Izberemo nakljuÄnega naslednika in Äe je boljÅ¡i od trenutnega stanja, to sprejmemo.

Ostale metode:

- [ ] Iskanje s spremenljivo sosesko
- [ ] Iskanje tabu
- [ ] Local beam search
- [ ] Genetski algoritem

[Hill climbing](https://www.youtube.com/watch?v=oSdPmxRCWws)

##### Genetski algoritem

Tvorimo populacijo k nakljuÄnih zaÄetnih stanj. Stanja so zakodirana v kromosome -
to je lahko npr. nek array vrednosti (recimo barva, pozicija x, pozicija y, ...).

Kodirano predstavitev reÅ¡itve imenujemo **genotip**, njeno dejansko interpretacijo
pa **fenotip**. Naslednjo generacijo osebkov tvorimo s pomoÄjo:

- [ ] Selekcije - izberemo najboljÅ¡e osebke
- [ ] KriÅ¾anja - izberemo dva najboljÅ¡a osebka in jih kriÅ¾amo
- [ ] Mutacije - izberemo nakljuÄnega osebka in ga mutiramo - mu nakljuÄno spremenimo
  neke lastnosti/vrednosti

Primer:
Genotip pri postavitvi kraljic na Å¡ahovnico bi lahko npr. bile Å¡tevilke od 1 do 8 
v arrayu, ki bi za posamezno vrstico predstavljale stolpec, v katerem je kraljica.

Imamo veÄ oblik selekcije:

- nakljuÄna selekcija,
- proporcionalna selekcija (verjetnost izbire kromosoma proporiconalna njegovi oceni),
- turnirska selekcija (izberemo nakljuÄno m < k osebkov in izberemo najboljÅ¡ega),
- selekcija po ranku (izberemo nakljuÄno proporcionalno indeksu kromosoma v sortiranem
  polju kromosomov),
- elitizem (izberemo najboljÅ¡e osebke in jih prenesemo v naslednjo generacijo),

KriÅ¾anje se zgodi z neko verjetnostjo pc. 

Oblike kriÅ¾anja:

- uniformno kriÅ¾anje (vsak par genov starÅ¡evskih kromosomov
  neodvisno izmenjamo z verjetnostjo pc),
- eno toÄkovno kriÅ¾anje (izberemo nakljuÄno toÄko in zamenjamo lastnosti na obeh kromosomih
  od te toÄke naprej),
- dvo toÄkovno kriÅ¾anje (izberemo dve nakljuÄni toÄki in zamenjamo lastnosti na obeh
  kromosomih med njima),

Mutacija z verjetnostjo pm za cel kromosom ali posamezno lastnost (gen).

[Genetski algoritem](https://www.youtube.com/watch?v=1i8muvzZkPw)

### Iskanje v igrah

Omejili se bomo na igre med dvema igralcema.

Pojmi:

- [x] Zero sum game - seÅ¡tevek rezultatov enega in drugega nasprotnika je 0
- [x] Igre s popolno informacijo - vemo vse o stanju igre
- [ ] Kompleksnost prostora stanj - Å¡tevilo poloÅ¾ajev dosegljivih iz zaÄetnega stanja
- [ ] Velikost drevesa iger - Å¡tevilo listo v drevesu - Å¡tevilo vseh razliÄnih iger

Cilj je iskanje Äim boljÅ¡e naslednje poteze. Uvedemo tudi ocenitveno funkcijo.
Preiskujemo s pomoÄjo drevesa igre â€“ v praksi se to ponavadi implementira z rekurzijo.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-21-41-34.png" width="700" alt="Drevo stanj">

Izvajamo iskanje po tem drevesu â€“ tako iÅ¡Äemo reÅ¡itev.

#### Minimax

Imamo igralca Min in Max, en maksimizira, drugi minimizira. Igralca se izmenjujeta.
Primer je igra s kriÅ¾ci in kroÅ¾ci. V listih dobimo statiÄne ocene poloÅ¾ajev â€“ to je v bistvu hevristika. Zmago nagradimo
z zelo visoko oceno.
Te ocene se nato prenaÅ¡ajo navzgor â€“ enkrat izberemo min, drugiÄ max â€“ odvisno, kateri igralec je na vrsti.

Implementacija z rekurzivnim iskanjem v globino.

<img src="https://davidblog.si/wp-content/uploads/2023/02/MIN_MAX1.jpg" alt="Drevesna struktura">

<a href="https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-4-alpha-beta-pruning/">Geeks for Geeks</a>

##### Gradivo iz vaj

```
Najprej bomo delali igre za dva igralca.
Na koncu je dobiÄek enak izgubi poraÅ¾enca.
Igro s popolnimi (Å¡ah, dama - vidimo vse na polju) in nepopolnimi informacijami (karte, ker ne vemo, kaj ima
nasprotnik).
Torej Äe nimamo popolnih informacij ne moremo vnaprej neÄesa raÄunat, pri popolni informaciji
pa to lahko. Lahko planiramo poteze brez potrebe po statistiki.

Pa se najprej osredotoÄimo na kriÅ¾ce in kroÅ¾ce. Igra se zagotovljeno konÄa po 9-tih potezah.
Zadevo definiramo kot iskalni problem. Skratka struktura z nekimi komponentami in po njej iÅ¡Äemo.
Vse skupaj formaliziramo, omejimo se na doloÄene igralce, poteze in kdaj je konec igre...

NaÅ¡e komponente:

* **ZaÄetno stanje** (obliko polja, koliko jih je, stanje - je prazno ponavadi) - le-tega rabimo, da vemo, kje zaÄeti.
  Z nekimi potezami delamo nova stanja.
* **Naslednje poteze** - funkcija prehoda med stanji. Torej formaliziramo, kako priti iz enega stanja v drugo.
  Rezultat funkcije je novo stanje. Z naslednjo potezo izberemo novo zaÄetno stanje in nadaljujemo
  z iskanjem naprej in ponavljamo.
* **ZakljuÄni pogoj igre** - kdaj je igre konec. Lahko je je konec, lahko je ni, lahko je neodloÄeno. -->
  MatematiÄna formulacija funkcije
* **Ocenitvena funkcija - hevristika** - tako razloÄimo med dobrimi in slabimi potezami. Izberemo si paÄ eno
  hevristiko - npr. gledamo katera poteza nam da najveÄ zmagovalnih potez.

Ko imamo vse te komponente lahko priÄnemo s strateÅ¡ko igro.

### Zakaj je to iskalni problem?

Ker je igranje igre v bistvu iskanje naÅ¡e najboljÅ¡e poteze. NaÅ¡a naloga je namreÄ iskanje najboljÅ¡ih potez.
Lahko razmiÅ¡ljamo tudi za veÄ potez naprej, ampak se omejimo na neko Å¡tevilo potez.

### Naprej...

Za nas so sovraÅ¾nikove dobre poteze slabe poteze, naÅ¡e dobre pa za nasprotnika slabe.

### Bistvo algoritma - drevo igre

Gre za vizualizacijo problema, ne rabimo dejanske drevesne strukture.

<img src="https://davidblog.si/wp-content/uploads/2023/02/MIN_MAX1.jpg" alt="Drevesna struktura">

Preprosto povedano, npr., iz zaÄetnega stanja praznega polja, lahko kriÅ¾ec postavimo na 9 razliÄnih polj,
nato kroÅ¾ec na preostalih 8 polj, nato... In dobimo drevesno strukturo.
Konkretno za kriÅ¾ce in kroÅ¾ce bi potlej skupno imeli 9! potez. To je kar veliko potez.
Na koncu pa paÄ pridemo do nekega zakljuÄnega stanja - seveda si Å¾elimo zmagovalnega stanja.

To je potlej naÅ¡ iskalni prostor. Z vsako izbiro pa je naÅ¡ iskalni prostor manjÅ¡i. Imamo izmenjujoÄe nivoje, enkrat
izbiramo mi, enkrat pa nasprotnik. Optimalni algoritem bi zgeneriral vsa moÅ¾na stanja in zatem izbiral
poteze, ki so zanj najbolj ugodne. Vse bi v naprej izraÄunal. To si pri takÅ¡ni igri lahko privoÅ¡Äimo, pri kompleksnih
igrah pa to ne gre, ker je raÄunsko prezahtevno.

### Pomembno

Pri rekurziji nikoli ne pozabi na konÄni pogoj! Rabimo neki if stavek.

NaÅ¡ega igralca simuliramo z maksimizatorjem,
nasprotnika pa minimizatorjem. NamreÄ mi igrami za zmago, za nasprotnika pa Å¾elimo, da izgubi.

Mi bomo gledalo nekaj potez naprej. Minimax izbere eno potezo in gleda Å¡e dve potezi naprej. Izbere
najboljÅ¡o potezo in pogleda nazaj, katera je naslednja poteza. Na naslednjo potezo vpliva ocenitvena
funkcija in trenutno stanje. HevristiÄno funkcija najdeÅ¡ na teams ali v uÄbeniku.

Vemo, da je najboljÅ¡a zaÄetna izbira za zmago sredina. Potemtakem to tudi izberemo preko hevristike.
Hevristiko lahko sredi igre tudi menjamo... Tako maksimiziramo moÅ¾nost zmage.
Potezam dodelimo uteÅ¾i in hevristiÄna funkcija nam vrne veÄjo vrednost za boljÅ¡e poteze.
Z igro zakljuÄimo, ko pridemo do globine 0.

Algoritem je v uÄbeniku na strani 68. ImaÅ¡ ga pa tudi na prosojnicah.

Algoritem v bistvu dela od spodaj navzgor. Ravno obratno kot mi govorimo.

## Algoritem Alfa-Beta

To je v bistvu nadgradnja prejÅ¡njega algoritma. Ta algoritem za razliko od prejÅ¡njega ne pregleda vsega.
Npr. na izbiro imamo tri slabe moÅ¾nosti - teh zato ne bomo preiskali, ker nam niÄ ne dajo.
Alfa-beta obrezovanje (pruning) - deluje tako, da obreÅ¾e nekatere veje z dodajanjem neskonÄnih vrednosti.
Recimo pri kakÅ¡ni igri kot je Å¡ah je tole kar obvezno.
Torej algoritem dopolnimo z alfo in beto, ki sta pa meji, za to ali gremo sploh gledat neko vejo naprej - skratka
odreÅ¾emo neko vejo - s pogojem prenehamo rekurzijo, tako da vrnemo.
ÄŒe pa ni rekurzije pa break.

<img src="https://davidblog.si/wp-content/uploads/2023/02/alpha-beta-pruning-step7.png" alt="Alpha-Beta Pruning">

Torej potlej damo recimo max na -Inf min pa na Inf. In max in min postavita neko mejo, do kje bomo Å¡e
preiskovali. In Äe se meji kriÅ¾ata, pomeni da nimamo boljÅ¡e reÅ¡itve.

UÄbenik str. 85.

Sprememba je tako zelo mala. Samo dodamo parametra alfa in beta. ÄŒe je val >= beta, zakljuÄimo...
EnaÄaj imamo zato, ker se lahko pojavi tudi enakost vrednosti in alfe (bete).
Za neskonÄno daj neko zelo veliko vrednost - neko vrednost, ki jo z hevristiko ne bomo dosegli.

### Note

Skrajno levo vedno pogledamo, ker je algoritem rekurziven.

### Optimalni alfa-beta rez

ÄŒe je vse poravnano spodaj, torej so uteÅ¾i urejene naraÅ¡ÄajoÄe.

### Navodila

Imej zgoraj izbiri teÅ¾avnosti. Potlej imej izbiro algoritma. Imej Å¡e Å¡tevec za zmage igralca, raÄunalnika,
izenaÄenje. Pa imej na sredini Å¡e polje za kriÅ¾ce in kroÅ¾ce.

### Zunanji viri

<a href="https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-4-alpha-beta-pruning/">Geeks for Geeks</a>
```

#### Negamaks

Ocene se razlikujejo le v predznaku â€“ hevristiÄna ocena je tako za oba igralca enaka. Zmaga ali izguba.

#### Algoritem Alpha-Beta

Tu pa uvedemo neko mejo in obrezovanje vej.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-21-53-59.png" alt="Alpha-Beta Pruning">

Recimo v primeru zgoraj:

1. Gremo od A do B do D.
2. Za D vidimo, da je 2, kar pomeni, da je zgornja meja za B (MIN) 2.
3. E ima veÄjo oceno, ga ne upoÅ¡tevamo.
4. Beta je tako 2.
5. Za A (MAX) je tako spodnja vrednost 2.
6. Alfa postane 2.
7. V C-ju ugotovimo, da je F = 1, torej beta = 1.
8. Ta poteza nam da vedeti, da bo A (MAX) zagotovo izbral B, ker ima le-ta viÅ¡jo spodnjo mejo --> alfa >= beta.
9. Za G nam je tako vseeno. To vejo obreÅ¾emo.

ÄŒe bi bilo drevo urejeno (najbolj levi naslednik v vsakem poddrevesu najugodnejÅ¡i za igralca na potezi), bi algoritem
ovrednotil pol toliko listov kot obiÄajni minimax.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-22-01-53.png" alt="Alpha-Beta Pruning" width="800">

Ta algoritem lahko implementiramo tudi z oknom alfa-beta.

#### MCTS

[Monte Carlo Tree Search](https://www.youtube.com/watch?v=UXW2yZndl7U)

Skratka glavna ideja je: izberemo naslednji list v drevesu stanj glede na podano formulo (oz. nakljuÄno). Za tisti list
nato izvedemo
simulacijo igre, ki nam doda informacije o tem, kako dober je ta list.

Nadgradnja je selekcija RAVE. Ideja tu je, da pri oceni poteze upoÅ¡tevamo vse simulacije, ne samo tiste, v keteri
i nastopa â€“ torej simulacije izvedene s to potezo.*

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-06-57-10.png" alt="RAVE" width="600">

* [RAVE](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)

### Okrepitveno uÄenje

Glavna ideja je, da se algoritem uÄi na podlagi izkuÅ¡enj. Za dobre akcije dobi nagrado, za slabe kazen.
Ta algoritem oz. model, ki se uÄi imenujemo **agent**.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/250px-Reinforcement_learning_diagram.svg.png" alt="Reinforcement Learning">

Ostale oblike strojnega uÄenja so:

* Nadzorovano uÄenje
* Ne-nadzorovano uÄenje
* Semi-nadzorovano uÄenje

Pojmi:

- [ ] okolje - v njem deluje agent
- [ ] stanje - v njem se nahaja agent
- [ ] akcija - agent lahko izvede akcijo
- [ ] pravilnik/strategija - doloÄa, kakÅ¡ne akcije lahko izvede agent
- [ ] nagrada - nagrada, ki jo dobi agent za izvedeno akcijo
- [ ] izkuÅ¡nja - stanje, akcija, nagrada, novo stanje

Cilj na dolgi rok je Äim veÄja nagrada. Stanje - akcija - nagrada.

Naloge agenta so lahko epizodne - to pomeni, da je potrebnih veÄ akcij, da se doseÅ¾e cilj.

Naloga je zvezna, Äe ni nekih jasnih meja med akcijami.

#### Markovski odloÄitveni procesi

[Markov Decision Process](https://www.youtube.com/watch?v=2iF9PRriA7w)

Glavna ideja je, da je naslednje stanje (prehod) odvisno samo od trenutnega stanja.

Primer je recimo, da poskuÅ¡amo priti do sluÅ¾be. Na voljo so nam avto, kolo in vlak.
ÄŒe gremo s kolesom, smo 100%, da bomo priÅ¡li v 45min. ÄŒe pa gremo z avtom, pa je recimo 10% moÅ¾nost, da
naletimo na prometno nesreÄo, kar pomeni, da bomo priÅ¡li v 60min. Je pa tudi 20% moÅ¾nost, da bomo priÅ¡li v 30min.
Skratka, uvedemo verjetnost. Z vlakom pa bomo npr. priÅ¡li v 35min vedno. Skratka prehajamo med stanji in uvedemo
verjetnosti.
Pri vlaku pa lahko dodamo Å¡e 10% moÅ¾nost za Äakalnico, v katero pa se lahko vraÄamo in Äe smo preveÄkrat Äakali,
gremo lahko tudi nazaj domov in nato recimo z avtom ali kolesom.

Uvedemo negotovost - verjetnost.

Te verjetnosti so predstavljene z **modelom okolja**.

IzraÄun verjetnosti:

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-16-07.png" alt="Markovski odloÄitveni procesi" width="600">

#### Komponente RL

* [ ] **Pravilnik** - obnaÅ¡anje agenta - katere akcije lahko izvede (npr. look-up tabela)
* [ ] **Signal nagrade** - zaÅ¾elenost stanja kot korist izvedene akcije
* [ ] **Funkcija vrednosti** - dolgoroÄna vrednost posameznega stanja
* [ ] **Model okolja** - predstavitev okolja - posnema delovanje okolja - omogoÄa planiranje - agent model lahko ima ali
  pa ga nima (trial error)

Ker so akcije zaporedne, je lahko okrepitveno uÄenje teÅ¾ko - ni nujno, da je neka akcija na dolgi rok dobra: **problem
dodelitve zaslug**.

Ker je interakcija z okoljem vzorÄena - torej nimamo popolnih informacij o okolju, je potrebno uporabiti
funkcijsko aproksimacijo pravilnika (npr. nevronska mreÅ¾a).

#### KriÅ¾ci in kroÅ¾ci

Minimaks predpostavi, da nasprotni igralec igra optimalno - najbolje. Ker pa mi nimamo popolnega modela okolja, tega
ne moremo predpostaviti oz. predvideti.

<img src="http://incompleteideas.net/book/ebook/figtmp0.png" alt="Tic-Tac-Toe" width="600">

Med igro izbiramo med najboljÅ¡imi potezami (izkoriÅ¡Äanje) in nakljuÄnimi potezami (raziskovanje).
Po vsaki poÅ¾reÅ¡ni potezi popravimo vrednost stanja s pred nasprotnikovo potezo (ker se korak pred tem o potezi
odloÄamo mi) tako, da jo pribliÅ¾amo vrednosti stanja sâ€™ po odgovoru

UÄenje s Äasovno razliko:

```
V(s) = V(s) + Î±Â·(V(sâ€™) â€“ V(s))
```

Alfa predstavlja hitrost uÄenja. Le-tega lahko sproti zmanjÅ¡ujemo do 0. Potlej konvergira, ampak preneha z uÄenjem.

#### ReciklaÅ¾ni robot

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-37-18.png" alt="ReciklaÅ¾ni robot" width="600">

#### PovraÄilo

Cilj agenta je maksimizirati povraÄilo, t.j. nagrado na dolgi rok. OznaÄimo ga z G. Je seÅ¡tevek vseh prihodnjih nagrad.
Uvedemo pa Å¡e zniÅ¾evalni faktor - tako so akcije, ki so blizu bolj pomembne od tistih v prihodnosti.

#### Pravilnik

Pove nam, kakÅ¡na je verjetnost, da bo agent v nekem stanju izvedel neko akcijo.

Funkcija vrednosti akcije? DoloÄa priÄakovano povraÄilo, Äe agent zaÄne v stanju s, izvede akcijo a in sledi pravilniku
Ï€.

Bellmanova enaÄba str. 34

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-44-16.png" alt="Bellmanova enaÄba" width="600">

#### Optimalni pravilnik

Skozi uÄenje Å¾elimo pravilnik izboljÅ¡ati. Pravilnik je boljÅ¡i, Äe velja

```
vÏ€1(s) â‰¥ vÏ€2(s) za vse sâˆˆğ’®
```

Vedno obstaja optimalni pravilnik. Za ocenitev pravilnika rabimo neko metodo (policy-evaluation).
Da bi izbirali med veÄ pravilniki, ni ravno uÄinkovito zato obstojeÄ pravilnik izboljÅ¡ujemo.

Optimalni pravilnik lahko poiÅ¡Äemo z algoritmom **posploÅ¡ene iteracije pravilnika**, ki ponavlja: 1) ocenitev pravilnika
(za trenuten pravilnik doloÄimo vrednost stanj in akcij) in 2) izboljÅ¡anje pravilnika. Ponavljamo do konvergence
pravilnika
(tj. dokler ne dobimo optimalnega pravilnika).

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-192732.png" alt="Optimalni pravilnik" width="600">

Po izboljÅ¡avi pravilnika izberemo najboljÅ¡e akcije. Tak pravilnik je deterministiÄen - v vsakem stanju izbere najboljÅ¡o
akcijo.
Za izboljÅ¡anje uporabljamo greedy metodo. Da pridemo do reÅ¡itve, niti ne rabimo Äakati do konvergence.

[Policy Iteration](https://www.youtube.com/watch?v=l87rgLg90HI)

#### Raziskovanje okolja

ÄŒe poznamo okolje - imamo model okolja - lahko planiramo. V resniÄnem svetu tega nimamo, zato uvedemo uÄenje s poskusi
in napakami.
Iteracija pravilnika je primer dinamiÄnega programiranja.

Raziskovanje vs. izkoriÅ¡Äanje (pozna okolje, ima neke informacije) - kompromis.

Cilj agenta pa je maksimizirati skupno nagrado (povraÄilo).

#### Strategije raziskovanja

PoÅ¾reÅ¡na (vedno najviÅ¡jo), nakljuÄna, Îµ-poÅ¾reÅ¡na (z verjetnostjo Îµ izberemo nakljuÄno akcijo), optimistiÄna
inicializacija
(zaÄetne vrednosti visoke, postopoma zniÅ¾ujemo), softmax (verjetnost izbire akcije je proporcionalna vrednosti akcije),
UCB.
UCB spominja na Monte-Carlo metode.

#### Optimizacija pravilnika RL

Iskanje optimalnega pravilnika -> ni modela okolja -> raziskovanje, izkoriÅ¡Äanje -> nagrada, povraÄilo -> iskanje
maksimalnega povraÄila -> ocenitev pravilnika/izboljÅ¡anje pravilnika.

Pravilnik lahko stohastiÄen (nakljuÄen) ali deterministiÄen (npr. greedy).

Problem napovedi - izraÄun vrednosti vseh stanj. PriÄakovano povraÄilo lahko simuliramo z Monte-Carlo metodo.
Vzamemo povpreÄje veÄ simulacij. Primerno za epizodne probleme: osveÅ¾itev stanj, akcij, pravilnika na koncu epizode.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-210410.png" alt="Monte-Carlo" width="600">

#### UÄenje s Äasovno razliko

Monte-Carlo metoda je neuÄinkovita, ker je preveÄ stohastiÄno. UÄenje s Äasovno razliko kombinira Monte-Carlo in
dinamiÄno programiranje. Monte-Carlo je dodano to, da se pravilnik posodablja z vsako akcijo.
Ocenitev pravilnika TD (ideja: z vsakim korakom t ocenimo priÄakovano povraÄilo do konca epizode):

```
vÏ€(s) = ğ”¼Ï€[Gt:T|St=s]
= ğ”¼Ï€[Rt+1 + Î³Â·Gt+1:T|St=s]
= ğ”¼Ï€[Rt+1 + Î³Â·vÏ€(St+1)|St=s]
```

bootstrapping

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-211018.png" alt="TD" width="600">

// TODO: The rest of the slides

### Predstavitev znanja z logiko

Pojmi:

- [ ] Izjave - dejstva
- [ ] Sklepanje - izpeljevanje novih dejstev
- [ ] Predstavitev znanja - predstavitev znanja v logiÄni obliki - logika
- [ ] Stavki ali formule - dejstva v jeziku logike - baza znanja
- [ ] Sintaksa
- [ ] Semantika
- [ ] ResniÄen, neresniÄen stavek
- [ ] LogiÄno sledi
- [ ] Algoritem sklepanja - iz baze znanja izpelje nova dejstva
- [ ] Smiselno sklepanje - ne sme biti nesmiselno - izpelje nekaj smiselnega iz baze znanja
- [ ] Poln - vse stvari, ki jih lahko izpeljemo iz baze znanja
- [ ] Boolova logika - logika, ki ima dve vrednosti: resniÄna in neresniÄna
- [ ] Atomarne izjave - nedeljive izjave
- [ ] Ne-atomarni stavki - deljivi na atomarne
- [ ] Konjunkcija, disjunkcija, implikacija, ekvivalenca, predpostavka, posledica, negacija
- [ ] Pravilnostna tabela
- [ ] Komutativnost, asociativnost, involucija (odstranitev negacije)
- [ ] Veljaven stavek - tavtologija
- [ ] Izpolnjiv stavek
- [ ] Dokaz s protislovjem - iz A sledi B
- [ ] Aksiomi -> teoremi = dokaz teorema
- [ ] Modus ponens, modus tollens

#### Resolucija

Uporablja se pri avtomatiÄnem dokazovanju. Deluje na stavkih, ki so klavzule (konjunkcija atomarnih izjav).
Velja, da lahko stavek P1 or P2 s stavkom Â¬P1 or Q1 zdruÅ¾imo v stavek P2 or Q1. Temu reÄemo resolventa.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-212834.png" alt="Resolucija" width="600">

#### Predikatna logika prvega reda

Sladki spomini na diskretne strukture.

Å tiri vrste simbolov:

- [ ] Simboli konstant - konÄni objekti
- [ ] Funkcijski simboli - preslikave med objekti
- [ ] Simboli spremenljivk - sploÅ¡ni objekti
- [ ] Simboli predikatov - relacije med objekti

Primer lastnosti: P(x, y) - x je oÄe y

Argumenti predikata so lahko zgoraj navedeni simboli. ReÄemo jim termi. Predikate poveÅ¾em z logiÄnimi operatorji (
konjunkcija, disjunkcija, implikacija, ekvivalenca, predpostavka, posledica, negacija).
vse spremenljivke morajo biti kvantificirane oz. vezane
(bound); nekvantificirane spremenljivke so proste (free)
â€“ stavki z vsemi vezanimi spremenljivkami so zaprti
(closed).

Univerzalnostni kvantifikator: za vse x velja P(x).
Existencialni kvantifikator: za kakÅ¡en koli x velja P(x).

Stavki morajo biti pravilno oblikovani.

Imamo Å¡e predikatno logiko drugega reda.

#### Substitucija

Gre za zamenjavo spremenljivk, to je vse. Notacija v/t pomeni, da vse pojavitve "t" v stavku zamenjamo z "v".

Kompozicija: TODO
