# Umetna inteligenca

## Pred vpraÅ¡anji

Sekcije s pojmi imajo na voljo check-marke. Le-te lahko oznaÄiÅ¡ tako, da v oglate oklepaje namesto presledka vstaviÅ¡
`x`. Na primer: `[x]` oznaÄi tole. Tako si lahko daÅ¡ zaznamke, kaj si Å¾e prebral.

## VpraÅ¡anja

#### Genetski algoritem

[Genetski algoritem](https://www.youtube.com/watch?v=1i8muvzZkPw)

### Iskanje v igrah

Omejili se bomo na igre med dvema igralcema.

Pojmi:

- [ ] Zero sum game
- [ ] Igre s popolno informacijo
- [ ] Kompleksnost prostora stanj
- [ ] Velikost drevesa iger

Cilj je iskanje Äim boljÅ¡e naslednje poteze. Uvedemo tudi ocenitveno funkcijo.
Preiskujemo s pomoÄjo drevesa igre â€“ v praksi se to ponavadi implementira z rekurzijo.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-21-41-34.png" width="700" alt="Drevo stanj">

Izvajamo iskanje po tem drevesu â€“ tako iÅ¡Äemo reÅ¡itev.

#### Minimax

Imamo igralca Min in Max, en maksimizira, drugi minimizira. Igralca se izmenjujeta.
Primer je igra s kriÅ¾ci in kroÅ¾ci. V listih dobimo statiÄne ocene poloÅ¾ajev â€“ to je v bistvu hevristika. Zmago nagradimo
z zelo visoko oceno.
Te ocene se nato prenaÅ¡ajo navzgor â€“ enkrat izberemo min, drugiÄ max â€“ odvisno, kateri igralec je na vrsti.

Implementacija z rekurzivnim iskanjem v globino.

<img src="https://davidblog.si/wp-content/uploads/2023/02/MIN_MAX1.jpg" alt="Drevesna struktura">

<a href="https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-4-alpha-beta-pruning/">Geeks for Geeks</a>

##### Gradivo iz vaj

```
Najprej bomo delali igre za dva igralca.
Na koncu je dobiÄek enak izgubi poraÅ¾enca.
Igro s popolnimi (Å¡ah, dama - vidimo vse na polju) in nepopolnimi informacijami (karte, ker ne vemo, kaj ima
nasprotnik).
Torej Äe nimamo popolnih informacij ne moremo vnaprej neÄesa raÄunat, pri popolni informaciji
pa to lahko. Lahko planiramo poteze brez potrebe po statistiki.

Pa se najprej osredotoÄimo na kriÅ¾ce in kroÅ¾ce. Igra se zagotovljeno konÄa po 9-tih potezah.
Zadevo definiramo kot iskalni problem. Skratka struktura z nekimi komponentami in po njej iÅ¡Äemo.
Vse skupaj formaliziramo, omejimo se na doloÄene igralce, poteze in kdaj je konec igre...

NaÅ¡e komponente:

* **ZaÄetno stanje** (obliko polja, koliko jih je, stanje - je prazno ponavadi) - le-tega rabimo, da vemo, kje zaÄeti.
  Z nekimi potezami delamo nova stanja.
* **Naslednje poteze** - funkcija prehoda med stanji. Torej formaliziramo, kako priti iz enega stanja v drugo.
  Rezultat funkcije je novo stanje. Z naslednjo potezo izberemo novo zaÄetno stanje in nadaljujemo
  z iskanjem naprej in ponavljamo.
* **ZakljuÄni pogoj igre** - kdaj je igre konec. Lahko je je konec, lahko je ni, lahko je neodloÄeno. -->
  MatematiÄna formulacija funkcije
* **Ocenitvena funkcija - hevristika** - tako razloÄimo med dobrimi in slabimi potezami. Izberemo si paÄ eno
  hevristiko - npr. gledamo katera poteza nam da najveÄ zmagovalnih potez.

Ko imamo vse te komponente lahko priÄnemo s strateÅ¡ko igro.

### Zakaj je to iskalni problem?

Ker je igranje igre v bistvu iskanje naÅ¡e najboljÅ¡e poteze. NaÅ¡a naloga je namreÄ iskanje najboljÅ¡ih potez.
Lahko razmiÅ¡ljamo tudi za veÄ potez naprej, ampak se omejimo na neko Å¡tevilo potez.

### Naprej...

Za nas so sovraÅ¾nikove dobre poteze slabe poteze, naÅ¡e dobre pa za nasprotnika slabe.

### Bistvo algoritma - drevo igre

Gre za vizualizacijo problema, ne rabimo dejanske drevesne strukture.

<img src="https://davidblog.si/wp-content/uploads/2023/02/MIN_MAX1.jpg" alt="Drevesna struktura">

Preprosto povedano, npr., iz zaÄetnega stanja praznega polja, lahko kriÅ¾ec postavimo na 9 razliÄnih polj,
nato kroÅ¾ec na preostalih 8 polj, nato... In dobimo drevesno strukturo.
Konkretno za kriÅ¾ce in kroÅ¾ce bi potlej skupno imeli 9! potez. To je kar veliko potez.
Na koncu pa paÄ pridemo do nekega zakljuÄnega stanja - seveda si Å¾elimo zmagovalnega stanja.

To je potlej naÅ¡ iskalni prostor. Z vsako izbiro pa je naÅ¡ iskalni prostor manjÅ¡i. Imamo izmenjujoÄe nivoje, enkrat
izbiramo mi, enkrat pa nasprotnik. Optimalni algoritem bi zgeneriral vsa moÅ¾na stanja in zatem izbiral
poteze, ki so zanj najbolj ugodne. Vse bi v naprej izraÄunal. To si pri takÅ¡ni igri lahko privoÅ¡Äimo, pri kompleksnih
igrah pa to ne gre, ker je raÄunsko prezahtevno.

### Pomembno

Pri rekurziji nikoli ne pozabi na konÄni pogoj! Rabimo neki if stavek.

NaÅ¡ega igralca simuliramo z maksimizatorjem,
nasprotnika pa minimizatorjem. NamreÄ mi igrami za zmago, za nasprotnika pa Å¾elimo, da izgubi.

Mi bomo gledalo nekaj potez naprej. Minimax izbere eno potezo in gleda Å¡e dve potezi naprej. Izbere
najboljÅ¡o potezo in pogleda nazaj, katera je naslednja poteza. Na naslednjo potezo vpliva ocenitvena
funkcija in trenutno stanje. HevristiÄno funkcija najdeÅ¡ na teams ali v uÄbeniku.

Vemo, da je najboljÅ¡a zaÄetna izbira za zmago sredina. Potemtakem to tudi izberemo preko hevristike.
Hevristiko lahko sredi igre tudi menjamo... Tako maksimiziramo moÅ¾nost zmage.
Potezam dodelimo uteÅ¾i in hevristiÄna funkcija nam vrne veÄjo vrednost za boljÅ¡e poteze.
Z igro zakljuÄimo, ko pridemo do globine 0.

Algoritem je v uÄbeniku na strani 68. ImaÅ¡ ga pa tudi na prosojnicah.

Algoritem v bistvu dela od spodaj navzgor. Ravno obratno kot mi govorimo.

## Algoritem Alfa-Beta

To je v bistvu nadgradnja prejÅ¡njega algoritma. Ta algoritem za razliko od prejÅ¡njega ne pregleda vsega.
Npr. na izbiro imamo tri slabe moÅ¾nosti - teh zato ne bomo preiskali, ker nam niÄ ne dajo.
Alfa-beta obrezovanje (pruning) - deluje tako, da obreÅ¾e nekatere veje z dodajanjem neskonÄnih vrednosti.
Recimo pri kakÅ¡ni igri kot je Å¡ah je tole kar obvezno.
Torej algoritem dopolnimo z alfo in beto, ki sta pa meji, za to ali gremo sploh gledat neko vejo naprej - skratka
odreÅ¾emo neko vejo - s pogojem prenehamo rekurzijo, tako da vrnemo.
ÄŒe pa ni rekurzije pa break.

<img src="https://davidblog.si/wp-content/uploads/2023/02/alpha-beta-pruning-step7.png" alt="Alpha-Beta Pruning">

Torej potlej damo recimo max na -Inf min pa na Inf. In max in min postavita neko mejo, do kje bomo Å¡e
preiskovali. In Äe se meji kriÅ¾ata, pomeni da nimamo boljÅ¡e reÅ¡itve.

UÄbenik str. 85.

Sprememba je tako zelo mala. Samo dodamo parametra alfa in beta. ÄŒe je val >= beta, zakljuÄimo...
EnaÄaj imamo zato, ker se lahko pojavi tudi enakost vrednosti in alfe (bete).
Za neskonÄno daj neko zelo veliko vrednost - neko vrednost, ki jo z hevristiko ne bomo dosegli.

### Note

Skrajno levo vedno pogledamo, ker je algoritem rekurziven.

### Optimalni alfa-beta rez

ÄŒe je vse poravnano spodaj, torej so uteÅ¾i urejene naraÅ¡ÄajoÄe.

### Navodila

Imej zgoraj izbiri teÅ¾avnosti. Potlej imej izbiro algoritma. Imej Å¡e Å¡tevec za zmage igralca, raÄunalnika,
izenaÄenje. Pa imej na sredini Å¡e polje za kriÅ¾ce in kroÅ¾ce.

### Zunanji viri

<a href="https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-4-alpha-beta-pruning/">Geeks for Geeks</a>
```

#### Negamaks

Ocene se razlikujejo le v predznaku â€“ hevristiÄna ocena je tako za oba igralca enaka. Zmaga ali izguba.

#### Algoritem Alpha-Beta

Tu pa uvedemo neko mejo in obrezovanje vej.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-21-53-59.png" alt="Alpha-Beta Pruning">

Recimo v primeru zgoraj:

1. Gremo od A do B do D.
2. Za D vidimo, da je 2, kar pomeni, da je zgornja meja za B (MIN) 2.
3. E ima veÄjo oceno, ga ne upoÅ¡tevamo.
4. Beta je tako 2.
5. Za A (MAX) je tako spodnja vrednost 2.
6. Alfa postane 2.
7. V C-ju ugotovimo, da je F = 1, torej beta = 1.
8. Ta poteza nam da vedeti, da bo A (MAX) zagotovo izbral B, ker ima le-ta viÅ¡jo spodnjo mejo --> alfa >= beta.
9. Ya G nam je tako vseeno. To vejo obreÅ¾emo.

ÄŒe bi bilo drevo urejeno (najbolj levi naslednik v vsakem poddrevesu najugodnejÅ¡i za igralca na potezi), bi algoritem
ovrednotil pol toliko listov kot obiÄajni minimax.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-30-22-01-53.png" alt="Alpha-Beta Pruning" width="800">

Ta algoritem lahko implementiramo tudi z oknom alfa-beta.

#### MCTS

[Monte Carlo Tree Search](https://www.youtube.com/watch?v=UXW2yZndl7U)

Skratka glavna ideja je: izberemo naslednji list v drevesu stanj glede na podano formulo (oz. nakljuÄno). Za tisti list
nato izvedemo
simulacijo igre, ki nam doda informacije o tem, kako dober je ta list.

Nadgradnja je selekcija RAVE. Ideja tu je, da pri oceni poteze upoÅ¡tevamo vse simulacije, ne samo tiste, v keteri
i nastopa â€“ torej simulacije izvedene s to potezo.*

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-06-57-10.png" alt="RAVE" width="600">

* [RAVE](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)

### Okrepitveno uÄenje

Glavna ideja je, da se algoritem uÄi na podlagi izkuÅ¡enj. Za dobre akcije dobi nagrado, za slabe kazen.
Ta algoritem oz. model, ki se uÄi imenujemo **agent**.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/250px-Reinforcement_learning_diagram.svg.png" alt="Reinforcement Learning">

Ostale oblike strojnega uÄenja so:

* Nadzorovano uÄenje
* Ne-nadzorovano uÄenje
* Semi-nadzorovano uÄenje

Pojmi:

- [ ] okolje - v njem deluje agent
- [ ] stanje - v njem se nahaja agent
- [ ] akcija - agent lahko izvede akcijo
- [ ] pravilnik/strategija - doloÄa, kakÅ¡ne akcije lahko izvede agent
- [ ] nagrada - nagrada, ki jo dobi agent za izvedeno akcijo
- [ ] izkuÅ¡nja - stanje, akcija, nagrada, novo stanje

Cilj na dolgi rok je Äim veÄja nagrada. Stanje - akcija - nagrada.

Naloge agenta so lahko epizodne - to pomeni, da je potrebnih veÄ akcij, da se doseÅ¾e cilj.

Naloga je zvezna, Äe ni nekih jasnih meja med akcijami.

#### Markovski odloÄitveni procesi

[Markov Decision Process](https://www.youtube.com/watch?v=2iF9PRriA7w)

Glavna ideja je, da je naslednje stanje (prehod) odvisno samo od trenutnega stanja.

Primer je recimo, da poskuÅ¡amo priti do sluÅ¾be. Na voljo so nam avto, kolo in vlak.
ÄŒe gremo s kolesom, smo 100%, da bomo priÅ¡li v 45min. ÄŒe pa gremo z avtom, pa je recimo 10% moÅ¾nost, da
naletimo na prometno nesreÄo, kar pomeni, da bomo priÅ¡li v 60min. Je pa tudi 20% moÅ¾nost, da bomo priÅ¡li v 30min.
Skratka, uvedemo verjetnost. Z vlakom pa bomo npr. priÅ¡li v 35min vedno. Skratka prehajamo med stanji in uvedemo
verjetnosti.
Pri vlaku pa lahko dodamo Å¡e 10% moÅ¾nost za Äakalnico, v katero pa se lahko vraÄamo in Äe smo preveÄkrat Äakali,
gremo lahko tudi nazaj domov in nato recimo z avtom ali kolesom.

Uvedemo negotovost - verjetnost.

Te verjetnosti so predstavljene z **modelom okolja**.

IzraÄun verjetnosti:

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-16-07.png" alt="Markovski odloÄitveni procesi" width="600">

#### Komponente RL

* [ ] **Pravilnik** - obnaÅ¡anje agenta - katere akcije lahko izvede (npr. look-up tabela)
* [ ] **Signal nagrade** - zaÅ¾elenost stanja kot korist izvedene akcije
* [ ] **Funkcija vrednosti** - dolgoroÄna vrednost posameznega stanja
* [ ] **Model okolja** - predstavitev okolja - posnema delovanje okolja - omogoÄa planiranje - agent model lahko ima ali
  pa ga nima (trial error)

Ker so akcije zaporedne, je lahko okrepitveno uÄenje teÅ¾ko - ni nujno, da je neka akcija na dolgi rok dobra: **problem
dodelitve zaslug**.

Ker je interakcija z okoljem vzorÄena - torej nimamo popolnih informacij o okolju, je potrebno uporabiti
funkcijsko aproksimacijo pravilnika (npr. nevronska mreÅ¾a).

#### KriÅ¾ci in kroÅ¾ci

Minimaks predpostavi, da nasprotni igralec igra optimalno - najbolje. Ker pa mi nimamo popolnega modela okolja, tega
ne moremo predpostaviti oz. predvideti.

<img src="http://incompleteideas.net/book/ebook/figtmp0.png" alt="Tic-Tac-Toe" width="600">

Med igro izbiramo med najboljÅ¡imi potezami (izkoriÅ¡Äanje) in nakljuÄnimi potezami (raziskovanje).
Po vsaki poÅ¾reÅ¡ni potezi popravimo vrednost stanja s pred nasprotnikovo potezo (ker se korak pred tem o potezi
odloÄamo mi) tako, da jo pribliÅ¾amo vrednosti stanja sâ€™ po odgovoru

UÄenje s Äasovno razliko:

```
V(s) = V(s) + Î±Â·(V(sâ€™) â€“ V(s))
```

Alfa predstavlja hitrost uÄenja. Le-tega lahko sproti zmanjÅ¡ujemo do 0. Potlej konvergira, ampak preneha z uÄenjem.

#### ReciklaÅ¾ni robot

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-37-18.png" alt="ReciklaÅ¾ni robot" width="600">

#### PovraÄilo

Cilj agenta je maksimizirati povraÄilo, t.j. nagrado na dolgi rok. OznaÄimo ga z G. Je seÅ¡tevek vseh prihodnjih nagrad.
Uvedemo pa Å¡e zniÅ¾evalni faktor - tako so akcije, ki so blizu bolj pomembne od tistih v prihodnosti.

#### Pravilnik

Pove nam, kakÅ¡na je verjetnost, da bo agent v nekem stanju izvedel neko akcijo.

Funkcija vrednosti akcije? DoloÄa priÄakovano povraÄilo, Äe agent zaÄne v stanju s, izvede akcijo a in sledi pravilniku
Ï€.

Bellmanova enaÄba str. 34

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-from-2023-03-31-14-44-16.png" alt="Bellmanova enaÄba" width="600">

#### Optimalni pravilnik

Skozi uÄenje Å¾elimo pravilnik izboljÅ¡ati. Pravilnik je boljÅ¡i, Äe velja

```
vÏ€1(s) â‰¥ vÏ€2(s) za vse sâˆˆğ’®
```

Vedno obstaja optimalni pravilnik. Za ocenitev pravilnika rabimo neko metodo (policy-evaluation).
Da bi izbirali med veÄ pravilniki, ni ravno uÄinkovito zato obstojeÄ pravilnik izboljÅ¡ujemo.

Optimalni pravilnik lahko poiÅ¡Äemo z algoritmom **posploÅ¡ene iteracije pravilnika**, ki ponavlja: 1) ocenitev pravilnika
(za trenuten pravilnik doloÄimo vrednost stanj in akcij) in 2) izboljÅ¡anje pravilnika. Ponavljamo do konvergence
pravilnika
(tj. dokler ne dobimo optimalnega pravilnika).

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-192732.png" alt="Optimalni pravilnik" width="600">

Po izboljÅ¡avi pravilnika izberemo najboljÅ¡e akcije. Tak pravilnik je deterministiÄen - v vsakem stanju izbere najboljÅ¡o
akcijo.
Za izboljÅ¡anje uporabljamo greedy metodo. Da pridemo do reÅ¡itve, niti ne rabimo Äakati do konvergence.

[Policy Iteration](https://www.youtube.com/watch?v=l87rgLg90HI)

#### Raziskovanje okolja

ÄŒe poznamo okolje - imamo model okolja - lahko planiramo. V resniÄnem svetu tega nimamo, zato uvedemo uÄenje s poskusi
in napakami.
Iteracija pravilnika je primer dinamiÄnega programiranja.

Raziskovanje vs. izkoriÅ¡Äanje (pozna okolje, ima neke informacije) - kompromis.

Cilj agenta pa je maksimizirati skupno nagrado (povraÄilo).

#### Strategije raziskovanja

PoÅ¾reÅ¡na (vedno najviÅ¡jo), nakljuÄna, Îµ-poÅ¾reÅ¡na (z verjetnostjo Îµ izberemo nakljuÄno akcijo), optimistiÄna
inicializacija
(zaÄetne vrednosti visoke, postopoma zniÅ¾ujemo), softmax (verjetnost izbire akcije je proporcionalna vrednosti akcije),
UCB.
UCB spominja na Monte-Carlo metode.

#### Optimizacija pravilnika RL

Iskanje optimalnega pravilnika -> ni modela okolja -> raziskovanje, izkoriÅ¡Äanje -> nagrada, povraÄilo -> iskanje
maksimalnega povraÄila -> ocenitev pravilnika/izboljÅ¡anje pravilnika.

Pravilnik lahko stohastiÄen (nakljuÄen) ali deterministiÄen (npr. greedy).

Problem napovedi - izraÄun vrednosti vseh stanj. PriÄakovano povraÄilo lahko simuliramo z Monte-Carlo metodo.
Vzamemo povpreÄje veÄ simulacij. Primerno za epizodne probleme: osveÅ¾itev stanj, akcij, pravilnika na koncu epizode.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-210410.png" alt="Monte-Carlo" width="600">

#### UÄenje s Äasovno razliko

Monte-Carlo metoda je neuÄinkovita, ker je preveÄ stohastiÄno. UÄenje s Äasovno razliko kombinira Monte-Carlo in
dinamiÄno programiranje. Monte-Carlo je dodano to, da se pravilnik posodablja z vsako akcijo.
Ocenitev pravilnika TD (ideja: z vsakim korakom t ocenimo priÄakovano povraÄilo do konca epizode):

```
vÏ€(s) = ğ”¼Ï€[Gt:T|St=s]
= ğ”¼Ï€[Rt+1 + Î³Â·Gt+1:T|St=s]
= ğ”¼Ï€[Rt+1 + Î³Â·vÏ€(St+1)|St=s]
```

bootstrapping

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-211018.png" alt="TD" width="600">

// TODO: The rest of the slides

### Predstavitev znanja z logiko

Pojmi:

- [ ] Izjave - dejstva
- [ ] Sklepanje - izpeljevanje novih dejstev
- [ ] Predstavitev znanja - predstavitev znanja v logiÄni obliki - logika
- [ ] Stavki ali formule - dejstva v jeziku logike - baza znanja
- [ ] Sintaksa
- [ ] Semantika
- [ ] ResniÄen, neresniÄen stavek
- [ ] LogiÄno sledi
- [ ] Algoritem sklepanja - iz baze znanja izpelje nova dejstva
- [ ] Smiselno sklepanje - ne sme biti nesmiselno - izpelje nekaj smiselnega iz baze znanja
- [ ] Poln - vse stvari, ki jih lahko izpeljemo iz baze znanja
- [ ] Boolova logika - logika, ki ima dve vrednosti: resniÄna in neresniÄna
- [ ] Atomarne izjave - nedeljive izjave
- [ ] Ne-atomarni stavki - deljivi na atomarne
- [ ] Konjunkcija, disjunkcija, implikacija, ekvivalenca, predpostavka, posledica, negacija
- [ ] Pravilnostna tabela
- [ ] Komutativnost, asociativnost, involucija (odstranitev negacije)
- [ ] Veljaven stavek - tavtologija
- [ ] Izpolnjiv stavek
- [ ] Dokaz s protislovjem - iz A sledi B
- [ ] Aksiomi -> teoremi = dokaz teorema
- [ ] Modus ponens, modus tollens

#### Resolucija

Uporablja se pri avtomatiÄnem dokazovanju. Deluje na stavkih, ki so klavzule (konjunkcija atomarnih izjav).
Velja, da lahko stavek P1 or P2 s stavkom Â¬P1 or Q1 zdruÅ¾imo v stavek P2 or Q1. Temu reÄemo resolventa.

<img src="https://davidblog.si/wp-content/uploads/2023/03/Screenshot-2023-03-31-212834.png" alt="Resolucija" width="600">

#### Predikatna logika prvega reda

Sladki spomini na diskretne strukture.

Å tiri vrste simbolov:

- [ ] Simboli konstant - konÄni objekti
- [ ] Funkcijski simboli - preslikave med objekti
- [ ] Simboli spremenljivk - sploÅ¡ni objekti
- [ ] Simboli predikatov - relacije med objekti

Primer lastnosti: P(x, y) - x je oÄe y

Argumenti predikata so lahko zgoraj navedeni simboli. ReÄemo jim termi. Predikate poveÅ¾em z logiÄnimi operatorji (
konjunkcija, disjunkcija, implikacija, ekvivalenca, predpostavka, posledica, negacija).
vse spremenljivke morajo biti kvantificirane oz. vezane
(bound); nekvantificirane spremenljivke so proste (free)
â€“ stavki z vsemi vezanimi spremenljivkami so zaprti
(closed).

Univerzalnostni kvantifikator: za vse x velja P(x).
Existencialni kvantifikator: za kakÅ¡en koli x velja P(x).

Stavki morajo biti pravilno oblikovani.

Imamo Å¡e predikatno logiko drugega reda.

#### Substitucija

Gre za zamenjavo spremenljivk, to je vse. Notacija v/t pomeni, da vse pojavitve "t" v stavku zamenjamo z "v".

Kompozicija: TODO
